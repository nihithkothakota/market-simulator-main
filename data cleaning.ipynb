{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8732001,"sourceType":"datasetVersion","datasetId":5241331}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-13T06:26:38.480067Z","iopub.execute_input":"2024-08-13T06:26:38.480568Z","iopub.status.idle":"2024-08-13T06:26:39.924290Z","shell.execute_reply.started":"2024-08-13T06:26:38.480529Z","shell.execute_reply":"2024-08-13T06:26:39.922549Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/software-reviews/meta_Software.jsonl\n/kaggle/input/software-reviews/Software.jsonl\n","output_type":"stream"}]},{"cell_type":"code","source":"'''\nimport json\nimport gc\n\nreviews=\"/kaggle/input/software-reviews/Software.jsonl\"\nproducts=\"/kaggle/input/software-reviews/meta_Software.jsonl\"\n\nwith open(reviews,'r') as file:\n    reviews_data = [json.loads(line) for line in file]\n\nwith open(products,'r') as file:\n    products_data = [json.loads(line) for line in file]\n\nreviews_df = pd.DataFrame(reviews_data)\nproducts_df = pd.DataFrame(products_data)\n\ndel reviews_data\ndel products_data\ngc.collect()\n\nreviews_df = reviews_df[['parent_asin', 'text']]\nproducts_df = products_df[['parent_asin', 'features', 'description']]\n\nmerged_df = pd.merge(reviews_df, products_df, on='parent_asin', how='inner')\noutput_df = merged_df[['parent_asin', 'features', 'description', 'text']]\n\noutput_file = 'merged_output.jsonl'\ncount=0\nwith open(output_file, 'w') as file:\n    for record in output_df.to_dict(orient='records'):\n        if(count%10000==0):\n            print(f\"Processed {count} records\")\n        file.write(json.dumps(record) + '\\n')\n        count+=1\n\nprint(f\"Data successfully merged and saved to {output_file}\")\n'''","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:26:46.815949Z","iopub.execute_input":"2024-08-13T06:26:46.816583Z","iopub.status.idle":"2024-08-13T06:26:46.828491Z","shell.execute_reply.started":"2024-08-13T06:26:46.816547Z","shell.execute_reply":"2024-08-13T06:26:46.827006Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'\\nimport json\\nimport gc\\n\\nreviews=\"/kaggle/input/software-reviews/Software.jsonl\"\\nproducts=\"/kaggle/input/software-reviews/meta_Software.jsonl\"\\n\\nwith open(reviews,\\'r\\') as file:\\n    reviews_data = [json.loads(line) for line in file]\\n\\nwith open(products,\\'r\\') as file:\\n    products_data = [json.loads(line) for line in file]\\n\\nreviews_df = pd.DataFrame(reviews_data)\\nproducts_df = pd.DataFrame(products_data)\\n\\ndel reviews_data\\ndel products_data\\ngc.collect()\\n\\nreviews_df = reviews_df[[\\'parent_asin\\', \\'text\\']]\\nproducts_df = products_df[[\\'parent_asin\\', \\'features\\', \\'description\\']]\\n\\nmerged_df = pd.merge(reviews_df, products_df, on=\\'parent_asin\\', how=\\'inner\\')\\noutput_df = merged_df[[\\'parent_asin\\', \\'features\\', \\'description\\', \\'text\\']]\\n\\noutput_file = \\'merged_output.jsonl\\'\\ncount=0\\nwith open(output_file, \\'w\\') as file:\\n    for record in output_df.to_dict(orient=\\'records\\'):\\n        if(count%10000==0):\\n            print(f\"Processed {count} records\")\\n        file.write(json.dumps(record) + \\'\\n\\')\\n        count+=1\\n\\nprint(f\"Data successfully merged and saved to {output_file}\")\\n'"},"metadata":{}}]},{"cell_type":"code","source":"import json\nfrom collections import defaultdict\n\n# Function to load JSONL files\ndef load_jsonl(file_path):\n    data = []\n    with open(file_path, 'r') as file:\n        for line in file:\n            data.append(json.loads(line))\n    return data\n\n# Load the datasets\nreviews = load_jsonl('/kaggle/input/software-reviews/Software.jsonl')\nproducts = load_jsonl('/kaggle/input/software-reviews/meta_Software.jsonl')\n\n# Create a mapping for products based on parent_asin\nproduct_details = {}\nfor product in products:\n    parent_asin = product['parent_asin']\n    product_details[parent_asin] = {\n        \"features\": product.get(\"features\", []),\n        \"description\": product.get(\"description\", [])\n    }\n\n# Create a mapping for reviews based on parent_asin\nproduct_reviews = defaultdict(list)\nfor review in reviews:\n    parent_asin = review['parent_asin']\n    product_reviews[parent_asin].append(review['text'])\n\n# Merge product details with reviews\nmerged_data = []\nfor parent_asin, details in product_details.items():\n    if parent_asin in product_reviews:\n        for review_text in product_reviews[parent_asin]:\n            merged_data.append({\n                \"parent_asin\": parent_asin,\n                \"features\": details[\"features\"],\n                \"description\": details[\"description\"],\n                \"review_text\": review_text\n            })\n\n'''            \n# Save the merged data to a new JSONL file\nprint(\"done\")\noutput_file = 'merged_data.jsonl'\ncount=0\nwith open(output_file, 'w') as file:\n    for entry in merged_data:\n        if count%10000==0:\n            print(f\"merged {count} files\")\n        file.write(json.dumps(entry) + '\\n')\n        count+=1\n        if count==1000000:\n            break\n\nprint(f\"Merged data saved to {output_file}\")\n'''","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:26:49.767772Z","iopub.execute_input":"2024-08-13T06:26:49.768211Z","iopub.status.idle":"2024-08-13T06:29:19.365026Z","shell.execute_reply.started":"2024-08-13T06:26:49.768149Z","shell.execute_reply":"2024-08-13T06:29:19.363449Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'            \\n# Save the merged data to a new JSONL file\\nprint(\"done\")\\noutput_file = \\'merged_data.jsonl\\'\\ncount=0\\nwith open(output_file, \\'w\\') as file:\\n    for entry in merged_data:\\n        if count%10000==0:\\n            print(f\"merged {count} files\")\\n        file.write(json.dumps(entry) + \\'\\n\\')\\n        count+=1\\n        if count==1000000:\\n            break\\n\\nprint(f\"Merged data saved to {output_file}\")\\n'"},"metadata":{}}]},{"cell_type":"code","source":"final_data=[]\ncount=0\nfor entry in merged_data:\n    if count%10000==0:\n        print(f\"{count} files appended\")\n    instruct=\"based on the given input write a review for the product\"\n    inp=\"features: '\"\n    f=entry[\"features\"]\n    for i in f:\n        inp+=i\n        inp+=\",\"\n    inp+=\"'; Description: '\"\n    d=entry[\"description\"]\n    for i in d:\n        inp+=i\n        inp+=','\n    inp+=\"'\"\n    out=entry[\"review_text\"]\n    row={\n        \"instruction\": instruct,\n        \"input\": inp,\n        \"output\": out\n    }\n    final_data.append(row)\n    count+=1\nprint(\"\")\nprint(\"appended all rows in the array\")","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:30:10.093329Z","iopub.execute_input":"2024-08-13T06:30:10.093745Z","iopub.status.idle":"2024-08-13T06:31:08.443895Z","shell.execute_reply.started":"2024-08-13T06:30:10.093712Z","shell.execute_reply":"2024-08-13T06:31:08.441931Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"0 files appended\n10000 files appended\n20000 files appended\n30000 files appended\n40000 files appended\n50000 files appended\n60000 files appended\n70000 files appended\n80000 files appended\n90000 files appended\n100000 files appended\n110000 files appended\n120000 files appended\n130000 files appended\n140000 files appended\n150000 files appended\n160000 files appended\n170000 files appended\n180000 files appended\n190000 files appended\n200000 files appended\n210000 files appended\n220000 files appended\n230000 files appended\n240000 files appended\n250000 files appended\n260000 files appended\n270000 files appended\n280000 files appended\n290000 files appended\n300000 files appended\n310000 files appended\n320000 files appended\n330000 files appended\n340000 files appended\n350000 files appended\n360000 files appended\n370000 files appended\n380000 files appended\n390000 files appended\n400000 files appended\n410000 files appended\n420000 files appended\n430000 files appended\n440000 files appended\n450000 files appended\n460000 files appended\n470000 files appended\n480000 files appended\n490000 files appended\n500000 files appended\n510000 files appended\n520000 files appended\n530000 files appended\n540000 files appended\n550000 files appended\n560000 files appended\n570000 files appended\n580000 files appended\n590000 files appended\n600000 files appended\n610000 files appended\n620000 files appended\n630000 files appended\n640000 files appended\n650000 files appended\n660000 files appended\n670000 files appended\n680000 files appended\n690000 files appended\n700000 files appended\n710000 files appended\n720000 files appended\n730000 files appended\n740000 files appended\n750000 files appended\n760000 files appended\n770000 files appended\n780000 files appended\n790000 files appended\n800000 files appended\n810000 files appended\n820000 files appended\n830000 files appended\n840000 files appended\n850000 files appended\n860000 files appended\n870000 files appended\n880000 files appended\n890000 files appended\n900000 files appended\n910000 files appended\n920000 files appended\n930000 files appended\n940000 files appended\n950000 files appended\n960000 files appended\n970000 files appended\n980000 files appended\n990000 files appended\n1000000 files appended\n1010000 files appended\n1020000 files appended\n1030000 files appended\n1040000 files appended\n1050000 files appended\n1060000 files appended\n1070000 files appended\n1080000 files appended\n1090000 files appended\n1100000 files appended\n1110000 files appended\n1120000 files appended\n1130000 files appended\n1140000 files appended\n1150000 files appended\n1160000 files appended\n1170000 files appended\n1180000 files appended\n1190000 files appended\n1200000 files appended\n1210000 files appended\n1220000 files appended\n1230000 files appended\n1240000 files appended\n1250000 files appended\n1260000 files appended\n1270000 files appended\n1280000 files appended\n1290000 files appended\n1300000 files appended\n1310000 files appended\n1320000 files appended\n1330000 files appended\n1340000 files appended\n1350000 files appended\n1360000 files appended\n1370000 files appended\n1380000 files appended\n1390000 files appended\n1400000 files appended\n1410000 files appended\n1420000 files appended\n1430000 files appended\n1440000 files appended\n1450000 files appended\n1460000 files appended\n1470000 files appended\n1480000 files appended\n1490000 files appended\n1500000 files appended\n1510000 files appended\n1520000 files appended\n1530000 files appended\n1540000 files appended\n1550000 files appended\n1560000 files appended\n1570000 files appended\n1580000 files appended\n1590000 files appended\n1600000 files appended\n1610000 files appended\n1620000 files appended\n1630000 files appended\n1640000 files appended\n1650000 files appended\n1660000 files appended\n1670000 files appended\n1680000 files appended\n1690000 files appended\n1700000 files appended\n1710000 files appended\n1720000 files appended\n1730000 files appended\n1740000 files appended\n1750000 files appended\n1760000 files appended\n1770000 files appended\n1780000 files appended\n1790000 files appended\n1800000 files appended\n1810000 files appended\n1820000 files appended\n1830000 files appended\n1840000 files appended\n1850000 files appended\n1860000 files appended\n1870000 files appended\n1880000 files appended\n1890000 files appended\n1900000 files appended\n1910000 files appended\n1920000 files appended\n1930000 files appended\n1940000 files appended\n1950000 files appended\n1960000 files appended\n1970000 files appended\n1980000 files appended\n1990000 files appended\n2000000 files appended\n2010000 files appended\n2020000 files appended\n2030000 files appended\n2040000 files appended\n2050000 files appended\n2060000 files appended\n2070000 files appended\n2080000 files appended\n2090000 files appended\n2100000 files appended\n2110000 files appended\n2120000 files appended\n2130000 files appended\n2140000 files appended\n2150000 files appended\n2160000 files appended\n2170000 files appended\n2180000 files appended\n2190000 files appended\n2200000 files appended\n2210000 files appended\n2220000 files appended\n2230000 files appended\n2240000 files appended\n2250000 files appended\n2260000 files appended\n2270000 files appended\n2280000 files appended\n2290000 files appended\n2300000 files appended\n2310000 files appended\n2320000 files appended\n2330000 files appended\n2340000 files appended\n2350000 files appended\n2360000 files appended\n2370000 files appended\n2380000 files appended\n2390000 files appended\n2400000 files appended\n2410000 files appended\n2420000 files appended\n2430000 files appended\n2440000 files appended\n2450000 files appended\n2460000 files appended\n2470000 files appended\n2480000 files appended\n2490000 files appended\n2500000 files appended\n2510000 files appended\n2520000 files appended\n2530000 files appended\n2540000 files appended\n2550000 files appended\n2560000 files appended\n2570000 files appended\n2580000 files appended\n2590000 files appended\n2600000 files appended\n2610000 files appended\n2620000 files appended\n2630000 files appended\n2640000 files appended\n2650000 files appended\n2660000 files appended\n2670000 files appended\n2680000 files appended\n2690000 files appended\n2700000 files appended\n2710000 files appended\n2720000 files appended\n2730000 files appended\n2740000 files appended\n2750000 files appended\n2760000 files appended\n2770000 files appended\n2780000 files appended\n2790000 files appended\n2800000 files appended\n2810000 files appended\n2820000 files appended\n2830000 files appended\n2840000 files appended\n2850000 files appended\n2860000 files appended\n2870000 files appended\n2880000 files appended\n2890000 files appended\n2900000 files appended\n2910000 files appended\n2920000 files appended\n2930000 files appended\n2940000 files appended\n2950000 files appended\n2960000 files appended\n2970000 files appended\n2980000 files appended\n2990000 files appended\n3000000 files appended\n3010000 files appended\n3020000 files appended\n3030000 files appended\n3040000 files appended\n3050000 files appended\n3060000 files appended\n3070000 files appended\n3080000 files appended\n3090000 files appended\n3100000 files appended\n3110000 files appended\n3120000 files appended\n3130000 files appended\n3140000 files appended\n3150000 files appended\n3160000 files appended\n3170000 files appended\n3180000 files appended\n3190000 files appended\n3200000 files appended\n3210000 files appended\n3220000 files appended\n3230000 files appended\n3240000 files appended\n3250000 files appended\n3260000 files appended\n3270000 files appended\n3280000 files appended\n3290000 files appended\n3300000 files appended\n3310000 files appended\n3320000 files appended\n3330000 files appended\n3340000 files appended\n3350000 files appended\n3360000 files appended\n3370000 files appended\n3380000 files appended\n3390000 files appended\n3400000 files appended\n3410000 files appended\n3420000 files appended\n3430000 files appended\n3440000 files appended\n3450000 files appended\n3460000 files appended\n3470000 files appended\n3480000 files appended\n3490000 files appended\n3500000 files appended\n3510000 files appended\n3520000 files appended\n3530000 files appended\n3540000 files appended\n3550000 files appended\n3560000 files appended\n3570000 files appended\n3580000 files appended\n3590000 files appended\n3600000 files appended\n3610000 files appended\n3620000 files appended\n3630000 files appended\n3640000 files appended\n3650000 files appended\n3660000 files appended\n3670000 files appended\n3680000 files appended\n3690000 files appended\n3700000 files appended\n3710000 files appended\n3720000 files appended\n3730000 files appended\n3740000 files appended\n3750000 files appended\n3760000 files appended\n3770000 files appended\n3780000 files appended\n3790000 files appended\n3800000 files appended\n3810000 files appended\n3820000 files appended\n3830000 files appended\n3840000 files appended\n3850000 files appended\n3860000 files appended\n3870000 files appended\n3880000 files appended\n3890000 files appended\n3900000 files appended\n3910000 files appended\n3920000 files appended\n3930000 files appended\n3940000 files appended\n3950000 files appended\n3960000 files appended\n3970000 files appended\n3980000 files appended\n3990000 files appended\n4000000 files appended\n4010000 files appended\n4020000 files appended\n4030000 files appended\n4040000 files appended\n4050000 files appended\n4060000 files appended\n4070000 files appended\n4080000 files appended\n4090000 files appended\n4100000 files appended\n4110000 files appended\n4120000 files appended\n4130000 files appended\n4140000 files appended\n4150000 files appended\n4160000 files appended\n4170000 files appended\n4180000 files appended\n4190000 files appended\n4200000 files appended\n4210000 files appended\n4220000 files appended\n4230000 files appended\n4240000 files appended\n4250000 files appended\n4260000 files appended\n4270000 files appended\n4280000 files appended\n4290000 files appended\n4300000 files appended\n4310000 files appended\n4320000 files appended\n4330000 files appended\n4340000 files appended\n4350000 files appended\n4360000 files appended\n4370000 files appended\n4380000 files appended\n4390000 files appended\n4400000 files appended\n4410000 files appended\n4420000 files appended\n4430000 files appended\n4440000 files appended\n4450000 files appended\n4460000 files appended\n4470000 files appended\n4480000 files appended\n4490000 files appended\n4500000 files appended\n4510000 files appended\n4520000 files appended\n4530000 files appended\n4540000 files appended\n4550000 files appended\n4560000 files appended\n4570000 files appended\n4580000 files appended\n4590000 files appended\n4600000 files appended\n4610000 files appended\n4620000 files appended\n4630000 files appended\n4640000 files appended\n4650000 files appended\n4660000 files appended\n4670000 files appended\n4680000 files appended\n4690000 files appended\n4700000 files appended\n4710000 files appended\n4720000 files appended\n4730000 files appended\n4740000 files appended\n4750000 files appended\n4760000 files appended\n4770000 files appended\n4780000 files appended\n4790000 files appended\n4800000 files appended\n4810000 files appended\n4820000 files appended\n4830000 files appended\n4840000 files appended\n4850000 files appended\n4860000 files appended\n4870000 files appended\n4880000 files appended\n\nappended all rows in the array\n","output_type":"stream"}]},{"cell_type":"code","source":"final_data=final_data[:1000000] #reduces ram usage\noutput_file=\"dataset.jsonl\"\ncount=0\nwith open(output_file,'w') as file:\n    for entry in final_data:\n        if count%10000==0:\n            print(f\"{count} rows in dataset\")\n        file.write(json.dumps(entry)+\"\\n\")\n        count+=1\nprint(\"\")\nprint(\"all rows in file\")","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:32:53.460217Z","iopub.execute_input":"2024-08-13T06:32:53.463206Z","iopub.status.idle":"2024-08-13T06:33:22.437043Z","shell.execute_reply.started":"2024-08-13T06:32:53.462587Z","shell.execute_reply":"2024-08-13T06:33:22.434690Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"0 rows in dataset\n10000 rows in dataset\n20000 rows in dataset\n30000 rows in dataset\n40000 rows in dataset\n50000 rows in dataset\n60000 rows in dataset\n70000 rows in dataset\n80000 rows in dataset\n90000 rows in dataset\n100000 rows in dataset\n110000 rows in dataset\n120000 rows in dataset\n130000 rows in dataset\n140000 rows in dataset\n150000 rows in dataset\n160000 rows in dataset\n170000 rows in dataset\n180000 rows in dataset\n190000 rows in dataset\n200000 rows in dataset\n210000 rows in dataset\n220000 rows in dataset\n230000 rows in dataset\n240000 rows in dataset\n250000 rows in dataset\n260000 rows in dataset\n270000 rows in dataset\n280000 rows in dataset\n290000 rows in dataset\n300000 rows in dataset\n310000 rows in dataset\n320000 rows in dataset\n330000 rows in dataset\n340000 rows in dataset\n350000 rows in dataset\n360000 rows in dataset\n370000 rows in dataset\n380000 rows in dataset\n390000 rows in dataset\n400000 rows in dataset\n410000 rows in dataset\n420000 rows in dataset\n430000 rows in dataset\n440000 rows in dataset\n450000 rows in dataset\n460000 rows in dataset\n470000 rows in dataset\n480000 rows in dataset\n490000 rows in dataset\n500000 rows in dataset\n510000 rows in dataset\n520000 rows in dataset\n530000 rows in dataset\n540000 rows in dataset\n550000 rows in dataset\n560000 rows in dataset\n570000 rows in dataset\n580000 rows in dataset\n590000 rows in dataset\n600000 rows in dataset\n610000 rows in dataset\n620000 rows in dataset\n630000 rows in dataset\n640000 rows in dataset\n650000 rows in dataset\n660000 rows in dataset\n670000 rows in dataset\n680000 rows in dataset\n690000 rows in dataset\n700000 rows in dataset\n710000 rows in dataset\n720000 rows in dataset\n730000 rows in dataset\n740000 rows in dataset\n750000 rows in dataset\n760000 rows in dataset\n770000 rows in dataset\n780000 rows in dataset\n790000 rows in dataset\n800000 rows in dataset\n810000 rows in dataset\n820000 rows in dataset\n830000 rows in dataset\n840000 rows in dataset\n850000 rows in dataset\n860000 rows in dataset\n870000 rows in dataset\n880000 rows in dataset\n890000 rows in dataset\n900000 rows in dataset\n910000 rows in dataset\n920000 rows in dataset\n930000 rows in dataset\n940000 rows in dataset\n950000 rows in dataset\n960000 rows in dataset\n970000 rows in dataset\n980000 rows in dataset\n990000 rows in dataset\n\nall rows in file\n","output_type":"stream"}]},{"cell_type":"code","source":"#for downloading the output\nimport os\nos.chdir(r'/kaggle/working')\nfrom IPython.display import FileLink\nFileLink(r'/kaggle/working/dataset.jsonl')","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:33:43.203370Z","iopub.execute_input":"2024-08-13T06:33:43.204077Z","iopub.status.idle":"2024-08-13T06:33:43.214938Z","shell.execute_reply.started":"2024-08-13T06:33:43.204043Z","shell.execute_reply":"2024-08-13T06:33:43.213322Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/dataset.jsonl","text/html":"<a href='/kaggle/working/dataset.jsonl' target='_blank'>/kaggle/working/dataset.jsonl</a><br>"},"metadata":{}}]}]}